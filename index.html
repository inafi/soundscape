<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <title>SoundScape</title>
    <link rel="icon" href="Pics/favicon.png">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
    <link rel="stylesheet" href="CSS/home.css">
    <link rel="stylesheet" href="CSS/flexmason.css">
    <link rel="stylesheet" href="CSS/fonts/fa/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="JS/flexmasonary.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/slick-carousel@1.8.1/slick/slick.min.js"></script>
    <script src="JS/home.js"></script>
    <script>
        if (/(android|bb\d+|meego).+mobile|avantgo|bada\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|Silk|lge|maemo|midp|mmp|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows (ce|phone)|xda|xiino/i
            .test(navigator.userAgent) ||
            /1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\/|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\/)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\/(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\/|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\/|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo|to(pl|sh)|ts(70|m\-|m3|m5)|tx\-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\-|your|zeto|zte\-/i
            .test(navigator.userAgent.substr(0, 4))) {
            $("head").append('<link rel="stylesheet" href="CSS/mobile.css">');
        } else {
            document.write('<script src="JS/scroll.js"><\/script>');
        }
    </script>
</head>

<body>
    <nav class="navbar fixed-top navbar-expand-xl">
        <div class="center">
            <a href="#home"><span>Home</span></a>
            <a href="#details"><span>Details</span></a>
            <a href="#product"><span>Features</span></a>
            <a href="#app"><span>App</span></a>
            <a href="#prototypes"><span>Prototypes</span></a>
            <a href="#faqs"><span>FAQs</span></a>
            <a href="#about"><span>Team</span></a>
        </div>

        <div class="wrap-icon">
            <a href="#home">
                <img src="Pics/blackring.png">
            </a>
            <div class="navbartoggler" expanded="false">
                <span></span>
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        <div class="collapse">
            <div class="navbar-nav">
                <a href="#home"><span>Home</span></a>
                <a href="#details"><span>Details</span></a>
                <a href="#product"><span>Features</span></a>
                <a href="#app"><span>App</span></a>
                <a href="#prototypes"><span>Prototypes</span></a>
                <a href="#faqs"><span>FAQs</span></a>
                <a href="#about"><span>Team</span></a>
            </div>
        </div>
    </nav>

    <div class="landing" id="home">
        <div class="row">
            <div>
                <div class="text-wrap">
                    <p class="title">SoundScape</p> <br>
                    <p class="label">The Next-Gen Hearing Assistant</p>
                    <a href="#details"><button>Learn More</button></a>
                </div>
            </div>
            <div>
                <img src="Pics/display/18motors/cad.png" class="cad">
            </div>
        </div>
    </div>

    <div class="overlay-cover">
    </div>

    <div class="disqus">
        <script id="dsq-count-scr" src="//soundscape-2.disqus.com/count.js" async></script>

        <div id="disqus_thread"></div>
        <script>
            /**
             *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
             *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
            /*
            var disqus_config = function () {
            this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
            };
            */
            (function () { // DON'T EDIT BELOW THIS LINE
                var d = document,
                    s = d.createElement('script');
                s.src = 'https://soundscape-2.disqus.com/embed.js';
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
        </script>
        <noscript>
            Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments
                powered by Disqus.
            </a>
        </noscript>
    </div>

    <div class="details" id="details">
        <p class="title">Project Details</p>
        <div class="row">
            <div class="col-c">
                <i class="fal fa-photo-video"></i>
                <p class="label">Showcase Video</p>
                <div class="overlay">
                    <iframe src="" name="https://www.youtube.com/embed/zJkN6GqvM40?enablejsapi=1" allowfullscreen="true"
                        mozallowfullscreen="true" webkitallowfullscreen="true" class="vid">
                    </iframe>
                </div>
            </div>
            <div class="col-c">
                <i class="fal fa-video"></i>
                <p class="label">Demo Video</p>
                <div class="overlay">
                    <iframe src="" name="https://www.youtube.com/embed/CuhOyZ71fu8?enablejsapi=1" allowfullscreen="true"
                        mozallowfullscreen="true" webkitallowfullscreen="true" class="vid">
                    </iframe>
                </div>
            </div>
            <div class="col-c">
                <i class="fal fa-align-center"></i>
                <p class="label">Abstract</p>
                <div class="overlay">
                    <p class="title">SoundScape: Real-Time 3D Sound Localization and Classification with Sensory
                        Substitution for the Deaf and Hard of Hearing</p>
                    <p class="desc">
                        Current devices geared towards the deaf and hard of hearing, such as hearing aids, struggle to
                        localize and transmit sounds to those with severe hearing impairments. Advanced devices, like
                        cochlear implants, are invasive and cost $30,000 to $50,000. Devices that classify sounds, such
                        as home alert systems, are not suited for mobile use and recognize a limited number of noises.
                        Our goal was to convey the directionality, pitch, amplitude, sound classification, and speech
                        recognition of multiple sound sources to those with hearing impairments through a low-cost
                        device. We localized sounds with the SRP-PHAT-HSDA algorithm calculated on incoming audio
                        captured by a 6-microphone array. We separated sounds through Geometric Source Separation (GSS)
                        and beamforming, allowing us to isolate 4 audio sources. We classified each source in real-time
                        using a stacked generalization ensemble trained on an augmented audio dataset. Background noise
                        is filtered through dynamic noise reduction for each source. The direction along with filtered
                        amplitude and frequency are transmitted to the user through vibration motors in a wearable
                        device worn around the shoulders while classification and speech recognition are displayed on a
                        watchOS app. SoundScape can reliably localize four sounds under 15.49 degrees of error, classify
                        sounds with 93.4% accuracy, and operate under 0.2 seconds of latency. The entire device costs
                        $60 to manufacture with inexpensive TPU filament. SoundScape is the first device assistive
                        listening device to separate, localize, and classify multiple sound sources with the potential
                        to protect 466 million people with hearing loss worldwide.
                    </p>
                </div>
                <!-- <p class="desc">Actively search for objects in your vicinity and know how far they are.</p> -->
            </div>
            <div class="col-c">
                <i class="fal fa-presentation"></i>
                <p class="label">Slides</p>
                <div class="overlay">
                    <iframe src=""
                        name="https://docs.google.com/presentation/d/e/2PACX-1vRfMaEXTavkM9OgRSl1r_-RsBMlFSyO8RLZ2BgyRB1oqcyuEKMu8kd6g-hq9BcTLvDcZjHC09SZSFnX/embed?start=false&loop=false&delayms=3000"
                        allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true" class="slides">
                    </iframe>
                </div>
                <!-- <p class="desc">Results are produced within seconds through our onboard processing.</p> -->
            </div>
            <div class="col-c">
                <i class="fal fa-sticky-note"></i>
                <p class="label">Research Paper</p>
                <div class="overlay">
                    <object data="Resources/ISEF Paper.pdf" type="application/pdf" width="100%" height="100%">
                    </object>
                </div>
                <!-- <p class="desc">Results are produced within seconds through our onboard processing.</p> -->
            </div>
        </div>
    </div>

    <div class="highlight">
        <div class="row">
            <div>
                <div class="img-wrap">
                    <img src="Pics/highlights/class.png">
                </div>
            </div>
            <div class="text" style="background-image: linear-gradient(to bottom right, #44cc89, #3971fb)">
                <p class="label" style="color: #fcbc3c;">Real-Time Sound Classification</p>
                <p class="desc" style="color: #000;">
                    We use a combination of state of the art computer vision models, optimized to run on our device.
                    Through our custom algorithms, we are able to increase the accuracy of the classifications by
                    deploying machine learning techniques.
                </p>
            </div>
        </div>
    </div>

    <div class="product row" id="product">
        <div class="left">
            <div class="feature">
                <div class="icon-wrap">
                    <i class="fal fa-map-marker-alt"></i>
                </div>
                <p class="label">Sound Localization</p>
                <p class="desc">
                    We perform 3D sound localization with ODAS, which can localize up to four sound sources
                    simultaneously.
                </p>
            </div>
            <div class="feature">
                <div class="icon-wrap">
                    <i class="fal fa-waveform"></i>
                </div>
                <p class="label">Sound Separation</p>
                <p class="desc">
                    Using Geometric Source Separation, we separate each localized sound source from each other with
                    limited interference.
                </p>
            </div>
            <div class="feature">
                <p class="label">Sound Classification</p>
                <p class="desc">
                    By using our stacked generalization ensemble, we classify each separated sound source produced from
                    GSS.
                </p>
                <div class="icon-wrap">
                    <i class="fal fa-folder-tree"></i>
                </div>
            </div>
        </div>
        <div class="img-wrap">
            <img src="Pics/display/18motors/features.png">
        </div>
        <div class="right">
            <div class="feature">
                <p class="label">Speech Recognition</p>
                <p class="desc">
                    We use Google's Speech Recognition API, the best speech recognition on the market, to identify human
                    speech.
                </p>
                <div class="icon-wrap">
                    <i class="fal fa-comments-alt"></i>
                </div>
            </div>
            <div class="feature">
                <div class="icon-wrap">
                    <i class="fal fa-microphone"></i>
                </div>
                <p class="label">Background Noise</p>
                <p class="desc">
                    Using our dynamic noise reduction algorithm, we remove background noise so that the user doesn't
                    feel it
                    in sensory substitution.
                </p>
            </div>
            <div class="feature">
                <p class="label">Sensory Substitution</p>
                <p class="desc">
                    We convey music and the feeling of sound through haptic motor vibration with sensory
                    substitution.
                </p>
                <div class="icon-wrap">
                    <i class="fal fa-allergies"></i>
                </div>
            </div>
        </div>
    </div>

    <hr>

    <div class="app" id="app">
        <div class="back-wrap">
            <img src="Pics/backgrounds/app.png">
        </div>
        <div class="row">
            <div class="img-wrap">
                <img src="Pics/app/main.png">
            </div>
            <div class="text-wrap">
                <p class="label">SoundScape App</p>
                <p class="desc">
                    The Atheia app has the same high-accuracy models and features as the physical device, just
                    optimized
                    for the iPhone framework. Utilizing Apple's revolutionary technology in depth measurements
                    and
                    Augmented Reality, the Atheia app simplistically conveys a user's environment for more
                    informed
                    user
                    decisions.
                </p>
                <!-- <a href="" target="_blank"><img src="Pics/app/apple.png"></a>
                <a href="" target="_blank"><img src="Pics/app/google.png"></a> -->
            </div>
        </div>
    </div>

    <div class="highlight">
        <div class="row">
            <div class="text" style="background-image: linear-gradient(to top left, #f52d53, #e09ce2)">
                <p class="label" style="color: #303548;">State of the Art Sound Localization</p>
                <p class="desc">
                    We use a combination of state of the art computer vision models, optimized to run on our device.
                    Through our custom
                    algorithms, we are able to increase the accuracy of the classifications by deploying machine
                    learning techniques.
                </p>
            </div>
            <div>
                <div class="img-wrap">
                    <img src="Pics/highlights/local.jpg">
                </div>
            </div>
        </div>
    </div>

    <div class="prototypes" id="prototypes">
        <div class="back-wrap">
            <img src="Pics/backgrounds/prototype.png">
        </div>
        <p class="title">Prototypes</p>
        <div class="row">
            <div class="col-12">
                <div class="slick">
                    <div class="item">
                        <div class="bg v5" style="background-image:url(Pics/prototypes/v5.png)">
                            <p class="label">V5</p>
                        </div>
                    </div>
                    <div class="item">
                        <div class="bg v4" style="background-image:url(Pics/prototypes/v4.png)">
                            <p class="label">V4</p>
                        </div>
                    </div>
                    <div class="item">
                        <div class="bg v3" style="background-image:url(Pics/prototypes/v3.png)">
                            <p class="label">V3</p>
                        </div>
                    </div>
                    <div class="item">
                        <div class="bg v2" style="background-image:url(Pics/prototypes/v2.png)">
                            <p class="label">V2</p>
                        </div>
                    </div>
                    <div class="item">
                        <div class="bg v1" style="background-image:url(Pics/prototypes/v1.png)">
                            <p class="label">V1</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="highlight">
        <div class="row">
            <div>
                <div class="img-wrap">
                    <img src="Pics/highlights/pcb.jpg">
                </div>
            </div>
            <div class="text" style="background-color: rgb(22, 22, 22);">
                <p class="label" style="color: #00cdcd;">Compact and Manufacturable Design</p>
                <p class="desc">
                    Through much user testing and feedback we have come up with a sleek and compact design to house all
                    of our electronic components. By allowing fast charging the device can handle hours of use time
                    after just minutes of charge.
                </p>
            </div>
        </div>
    </div>

    <div class="bottompage">
        <div class="back-wrap">
            <img src="Pics/backgrounds/about.png">
        </div>

        <div class="faqs" id="faqs">
            <p class="title">Frequently Asked Questions</p>
            <!-- <div class="grid">
                <div>
                    <div>
                        <p class="label">How much will it cost?</p>
                        <p class="desc">
                            SoundScape will only cost $60. It is printed with inexpensive but flexible TPU filament
                            which
                            can be scaled to an injection mold for mass-production. Other parts, like the Raspberry Pi
                            and
                            Teensy 4.0, total to under $45.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">What sounds can it classify and how many?</p>
                        <p class="desc">
                            SoundScape can classify 50 sounds as well as recognize speech. These include emergency
                            sounds,
                            such as a baby crying or a car's engine, to common sounds like rain or footsteps. The user
                            can
                            choose specific sounds to receive a notification for.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">Do you plan to use a server going forward?</p>
                        <p class="desc">
                            By using bluetooth streaming to transmit audio from the microphone array to the phone, we
                            can
                            run computational tasks on a phone. We can compile our computer
                            vision models on CoreML to utilize an iPhone's neural engine.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">How does background noise filtering work?</p>
                        <p class="desc">
                            Noise reduction is done by first recording a 0.5 second audio clip from an initial interval
                            of
                            10 seconds. Then we use fourier transforms to extract the frequencies from the background
                            noise
                            to produce a mask. Then we use the mask to filter those frequencies from the signal audio.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">How have you accounted for different environments?</p>
                        <p class="desc">
                            We do this in two ways: background noise reduction and data augmentation. Dynamically reducing background noise allows us to adapt to a variety of environments. Data augmentation allows our classifier to account for outdoors environments.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">Does your system work in real-time?</p>
                        <p class="desc">
                            Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
                            incididunt ut labore et dolore magna
                            aliqua.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">Why do you do sound classification?</p>
                        <p class="desc">
                            Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
                            incididunt ut labore et dolore magna
                            aliqua.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">Where and how have you tested this?</p>
                        <p class="desc">
                            We've tested indoors, outdoors, and with varying degrees of background noise. We found
                            that
                            indoors, with constant background noise at 20 dB, we could localize sounds up to 35 ft
                            away.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">In your motor pairs, how did you decide when to turn on the red or green
                            LED?</p>
                        <p class="desc">
                            In each pair the red LED is for low frequencies and the green for high. We determined
                            the
                            threshold frequency by calculating the average median frequency for each audio sample in
                            the
                            ESC-50 dataset.
                        </p>
                    </div>
                </div>
            </div> -->
            <div class="grid">
                <div>
                    <div>
                        <p class="label">How much will it cost?</p>
                        <p class="desc">
                            SoundScape will only cost $60. It is printed with inexpensive but flexible TPU filament
                            which
                            can be scaled to an injection mold for mass-production. Other parts, like the Raspberry
                            Pi
                            and
                            Teensy 4.0, total to under $45.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">What sounds can it classify and how many?</p>
                        <p class="desc">
                            SoundScape can classify 50 sounds as well as recognize speech. These include emergency
                            sounds,
                            such as a baby crying or a car's engine, to common sounds like rain or footsteps. The
                            user
                            can
                            choose specific sounds to receive a notification for.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">Do you plan to use a server going forward?</p>
                        <p class="desc">
                            By using bluetooth streaming to transmit audio from the microphone array to the phone,
                            we
                            can
                            run computational tasks on a phone. We can compile our computer
                            vision models on CoreML to utilize an iPhone's neural engine.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">How does background noise filtering work?</p>
                        <p class="desc">
                            Noise reduction is done by first recording a 0.5 second audio clip from an initial
                            interval
                            of
                            10 seconds. Then we use fourier transforms to extract the frequencies from the
                            background
                            noise
                            to produce a mask. Then we use the mask to filter those frequencies from the signal
                            audio.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">How have you accounted for different environments?</p>
                        <p class="desc">
                            We do this in two ways: background noise reduction and data augmentation. Dynamically
                            reducing background noise allows us to adapt to a variety of environments. Data
                            augmentation allows our classifier to account for outdoors environments.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">Does your system work in real-time?</p>
                        <p class="desc">
                            Yes - the entire system has a latency under 0.2 seconds. We do this with GPU
                            acceleration using CuPy and classifying audio cumulatively, similar to how voice
                            assistants like Siri work.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">Why do you do sound classification?</p>
                        <p class="desc">
                            People who are deaf and hard of hearing rely on their eyes to identify what made a
                            sound. However, sound sources are often out of sight, such as a baby crying or a car
                            incoming behind you. In such cases, our users need to differentiate between "emergency"
                            sounds - cars, babies - and less urgent ones, like wind or rain.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">Where and how have you tested this?</p>
                        <p class="desc">
                            We've tested indoors, outdoors, and with varying degrees of background noise. We found
                            that
                            indoors, with constant background noise at 20 dB, we could localize sounds up to 35 ft
                            away.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">In your motor pairs, how did you decide when to turn on the red or green
                            LED?</p>
                        <p class="desc">
                            In each pair the red LED is for low frequencies and the green for high. We determined
                            the
                            threshold frequency by calculating the average median frequency for each audio sample in
                            the
                            ESC-50 dataset.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">How do you track sound sources?</p>
                        <p class="desc">
                            Using localization data from the SRP-PHAT-HSDA model, we know the direction of arrival for
                            each sound source. We then determine if the sound source is one of four states: static,
                            constant velocity, accelerating, or a new source. Then we use a 3D Kalman filter, called
                            M3K, to accurately track the sound source over time.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">How do you localize multiple sound sources?</p>
                        <p class="desc">
                            Each microphone has a set time difference of arrival which offsets to the generated audio
                            wave. So for every audio signal, the microphone array offsets the sound wave such that the
                            sound waves generated by every mic are in sync for the same sound source. This applies when
                            multiple audio signals coming from different directions are added. So if the
                            same audio signal from one mic is detected on another, the offset sound waves should almost
                            entirely be in sync. This allows the SRP model to filter out repeating sound sources.
                        </p>
                    </div>
                </div>
                <div>
                    <div>
                        <p class="label">What made you choose the four base models?</p>
                        <p class="desc">
                            We chose the models based on three criteria: accuracy, run-time, and diversity in terms of
                            classification methods. Our goal was to incorporate each model into the meta-classifier, so
                            having variation in terms of how they classify audio data increased the accuracy of the
                            meta-classifier. The 4 models we chose, Densenet, Resnet18, Cnn10, and Talnet, had some of
                            the highest accuracies on other sound classification datasets such as ESCS10 and the Urban8k
                            dataset.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <hr>

        <div class="about" id="about">
            <p class="title">Meet The Team</p>
            <p class="subtitle">We are high school juniors from TJHSST hoping to improve lives.</p>
            <div class="row">
                <div>
                    <div class="img-wrap">
                        <img src="Pics/team/eugene.jpg">
                    </div>
                    <p class="label">Eugene Choi</p>
                    <p class="desc">Co-Founder</p>
                </div>
                <div>
                    <div class="img-wrap">
                        <img src="Pics/team/nafi.png">
                    </div>
                    <p class="label">Irfan Nafi</p>
                    <p class="desc">Co-Founder</p>
                </div>
                <div>
                    <div class="img-wrap">
                        <img src="Pics/team/raffu.jpeg">
                    </div>
                    <p class="label">Raffu Khondaker</p>
                    <p class="desc">Co-Founder</p>
                </div>
            </div>
        </div>
    </div>
    <!-- <div class="b6" id="contact">
        <p class="title">Contact Us</p>
    </div> -->

    <div class="foot-wrap">
        <footer>
            <p class="copy" id="contact">
                Made with <i class="fa fa-heart"></i> by Irfan Nafi <br>
                Copyright © 2021 SoundScape. All rights reserved.
            </p>
        </footer>
    </div>
</body>
<!-- 
Developed by Irfan Nafi
-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-G4LFGS46NR"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-G4LFGS46NR');
</script>

</html>